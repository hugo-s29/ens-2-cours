\documentclass[./main]{subfiles}

\begin{document}
  \chapter{A perfectly secure symmetric encryption scheme: ONE-TIME PAD}

  This encryption scheme achieves information-theoric security.

  \begin{defn}[Symmetric encryption]
    Let $\mathcal{K}$ be a key space, $\mathcal{P}$ be a plain-text space and let $\mathcal{C}$ be a ciphertext space
    These three spaces are finite spaces.

    A \textit{symmetric encryption} scheme over $(\mathcal{K}, \mathcal{P}, \mathcal{C})$ is a tuple of three algorithms $(\mathrm{KeyGen}, \mathrm{Enc}, \mathrm{Dec})$ :
    \begin{itemize}
      \item $\mathrm{KeyGen}$ provides a sample $k$ of $\mathcal{K}$;
      \item $\mathrm{Enc} : \mathcal{K} \times \mathcal{P} \to \mathcal{C}$;
      \item $\mathrm{Dec} : \mathcal{K} \times \mathcal{C} \to \mathcal{P}$.
    \end{itemize}
    Without loss of generality, we will assume that $\operatorname{im} \mathrm{Enc} = \mathcal{C}$.
    We want to ensure \textbf{Correctness}: for any key $k \in \mathcal{K}$ and message $m \in \mathcal{P}$, we have that:
  \[
    \mathrm{Dec}(k, \mathrm{Enc}(k, m)) = m
    .\]

    The elements $m$ and $k$ are independent random variables and all the elements in $\mathcal{K}$ and $\mathcal{P}$ have non-zero probability.
  \end{defn}

  \begin{rmk}
    The algorithm $\mathrm{Enc}$ could (and should\footnote{If the algorithm is deterministic, if we see two identical ciphers we know that the messages are identical, and this can be seen as a vulnerability of this protocol.}) be probabilistic.
    However, the algorithm $\mathrm{Dec}$ is deterministic.
    
    So far, we did not talk about efficiency of these algorithms.
  \end{rmk}

  \begin{defn}[Shannon, 1949]
    A symmetric encryption scheme is said to have \textit{perfect security} whenever, for any $\bar{m}$ and any $\bar{c}$, 
    \[
      \Pr_{k,m}[m = \bar{m}  \mid \mathrm{Enc}_k(m) = \bar{c}] = \Pr_{m}[m = \bar{m}]
    .\] 
  \end{defn}

  The intuition is that knowing the encrypted message tells me \textit{nothing} about the message.

  \begin{lem}[Shannon]
    Given a symmetric encryption scheme $(\mathrm{KeyGen}, \mathrm{Enc}, \mathrm{Dec})$ has perfect security then $|\mathcal{K}| \ge |\mathcal{P}|$.
  \end{lem}
  \begin{prv}
    Let $\bar{c} \in \mathcal{C}$ and define \[
    \mathcal{S} := \{\bar{m} \in \mathcal{P}  \mid \exists \bar{k} \in \mathcal{K}, \bar{m} = \mathrm{Dec}(\bar{k}, \bar{c})\}
    .\]
    Let $N := |\mathcal{S}|$.
    We have that $N \le |\mathcal{K}|$ as $\mathrm{Dec}$ is deterministic.
    We also have that $N \le |\mathcal{P}|$ as $\mathcal{S} \subseteq \mathcal{P}$.
    Finally, assume $N < |\mathcal{P}|$.
    This means, there exists $\bar{m} \in \mathcal{P}$ such that $\bar{m} \not\in \mathcal{S}$.
    Then, 
    \[
      \Pr[m = \bar{m}  \mid \mathrm{Enc}_k(m) = \bar{c}] = 0
    ,\] 
    but by assumption, $\Pr[m = \bar{m}] \neq 0$.
    So this is not a perfectly secure scheme.
    We can conclude that \[
    N = |\mathcal{P}| \le |\mathcal{K}|
    .\] 
  \end{prv}

  \begin{exm}[One-Time PAD]
    Let $\mathcal{K} = \mathcal{C} = \mathcal{P} = \{0, 1\}^{\ell}$.
    Here are the algorithms used:
    \begin{itemize}
      \item $\mathrm{KeyGen}$ samples from $\mathcal{U}(\{0,1\}^\ell)$.
      \item $\mathrm{Enc}(k, m)$ we compute the XOR $c = m \oplus k$.
      \item  $\mathrm{Dec}(k,m)$ we compute the XOR $m = c \oplus k$.
    \end{itemize}
  \end{exm}

  \begin{thm}
    The One-Time PAD is a perfectly-secure symmetric encryption.
  \end{thm}
  \begin{prv}
    \begin{description}
      \item[Correctness.]
        We have that
        \[
        \mathrm{Dec}(k, \mathrm{Enc}(k, m)) = k \oplus k \oplus m = m
        .\]
      \item[Security.]
        We have, by independence of $m$ and $k$ we have that
        \begin{align*}
          \Pr[m = \bar{m}  \mid \mathrm{Enc}(k, m) = \bar{c}]
          &= \Pr[m = \bar{m}  \mid k \oplus m = \bar{c} ]\\
          &= \Pr[m = \bar{m}]
        .\end{align*}
    \end{description}
  \end{prv}

  \begin{rmk}
    This example is not practical:
    \begin{itemize}
      \item keys need to be larger than the message;
      \item you cannot encrypt twice: for example, $c_1 = m_1 \oplus k$ and $c_2 = m_2 \oplus k$, then we have $c_1 \oplus c_2 = m_1 \oplus m_2$.
    \end{itemize}
    This last part is why that protocol is called a \textit{One-Time secure encryption}.
  \end{rmk}

  We want to be able to encrypt arbitrarily long messages!
  We will have to make a trade-off and we choose to not care about \textit{perfect} security.
  Why? In real life, we don't care about proving that something is proven to be absolutely infeasible, we only want to believe it is infeasible in practice.
  \begin{center}
  \textbf{\textit{Computational complexity} is sufficient in practice.}
  \end{center}
  Let us be more precise.

  \begin{defn}
    Let $\mathcal{D}_0$ and $\mathcal{D}_1$ be two distributions over $\{0,1\}^n$.

    An algorithm $\mathcal{A} : \{0,1\}^n \to \{0,1\}$ is called a \textit{distinguisher} between $\mathcal{D}_0$ and $\mathcal{D}_1$. We define its \textit{distinguishing advantage} as:
    \[
      \mathrm{Adv}_\mathcal{A} := \big| \underbrace{\Pr_{x \gets \mathcal{D}_1} [\mathcal{A}(X) = 1]}_{ \mathclap{\substack{\text{probability of}\\ \text{being right}}} } - \underbrace{\Pr_{x \gets \mathcal{D}_0} [\mathcal{A}(X) = 1]}_{\mathclap{\substack{\text{probability of}\\ \text{being mistaken}}}} \big|
    .\] 

    We say that $\mathcal{D}_0$ and $\mathcal{D}_1$ are \textit{computationally indistinguishable} if for any efficient distinguisher $\mathcal{A}$ its advantage $\mathrm{Adv}_\mathcal{A}$ is small.
  \end{defn}

  This definition is not very formal yet, we have not defined "efficient" and "small."
  This can be formalized by introducing a parameter $\lambda \in \mathds{N}$ called the \textit{security parameter}.

  \begin{defn}
    Let $(\mathcal{D}_{0, \lambda})_{\lambda \in \mathds{N}}$ and $(\mathcal{D}_{1, \lambda})_{\lambda \in \mathds{N}}$ be two distributions over $\{0,1\}^{n(\lambda)}$ for a non-decreasing polynomial $n(\lambda)$.
    The value of $\lambda \in \mathds{N}$ is called the \textit{security parameter}.

    An algorithm $\mathcal{A} : \{0,1\}^{n(\lambda)} \to \{0,1\}$ is called a \textit{distinguisher} between the distributions $\mathcal{D}_{0, \lambda}$ and $\mathcal{D}_{1, \lambda}$. We define its \textit{distinguishing advantage} as:
    \[
      \mathrm{Adv}_\mathcal{A}(\lambda) := \big| \underbrace{\Pr_{x \gets \mathcal{D}_{1, \lambda}} [\mathcal{A}(X) = 1]}_{ \mathclap{\substack{\text{probability of}\\ \text{being right}}} } - \underbrace{\Pr_{x \gets \mathcal{D}_{0, \lambda}} [\mathcal{A}(X) = 1]}_{\mathclap{\substack{\text{probability of}\\ \text{being mistaken}}}} \big|
    .\] 

    We say that $\mathcal{D}_{0, \lambda}$ and $\mathcal{D}_{1, \lambda}$ are \textit{computationally indistinguishable} if for any distinguisher $\mathcal{A}$ running in $\mathrm{O}(\lambda^c)$ for some $c > 0$\footnote{This means it is polynomial in $\lambda$.} its advantage $\mathrm{Adv}_\mathcal{A}$ is a $\mathrm{o}(1 / \lambda^c)$ for some $c > 0$.\footnote{This means it is negligible in terms of $\lambda$, which we will write $\mathrm{negl}(\lambda)$.}
  \end{defn}
  
  Our goal now is to extend the One-Time PAD to messages $m$ larger than the key $k$.
  We want to construct some function $G$ that takes as input the key $k \in \{0,1\}^n$ and expend it to a string $G(k) \in \{0,1\}^\ell$  for some $\ell > k$ that is computationally hard to distinguish from a uniform random string.
  This is called a \textit{PGR} or \textit{pseudo-random generator}.

  \begin{defn}
    A \textit{pseudo-random generator} is a pair of poly-time algorithms $(\mathrm{Setup}, G)$ such that:
    \begin{itemize}
      \item $\mathrm{Setup}$ is an algorithm that takes as input a security parameter $\lambda$ (taken as a string $1^\lambda$ of length $\lambda$, \textit{i.e.}\ we write $\lambda$ in unary) and returns a public parameter;
      \item $G_\lambda : \{0,1\}^{n(\lambda)} \to \{0,1\}^{\ell(\lambda)}$ is an algorithm which takes a string $k$ of length $n(\lambda)$ and return a string $G(k)$ of length $\ell(\lambda)$ with $\ell(\lambda) > n(\lambda)$.
    \end{itemize}
    such that
    \begin{itemize}
      \item $G$ is deterministic;
      \item $\ell(\lambda) > n(\lambda)$ (we say that it is \textit{expanding})
      \item the distributions $\{\mathcal{U}(\{0,1\}^{\ell(\lambda)})\}_{\lambda \in \mathds{N}}$ and $\{G(\mathcal{U}(\{0,1\} ^{n(\lambda)}))\}_{\lambda \in \mathds{N}}$ are computationally indistinguishable (we call it \textit{pseudo-randomness}).
    \end{itemize}
  \end{defn}
  
  Another way of defining a pseudo-random generator is with \textit{unpredictability} instead of \textit{pseudo-randomness}.
  \begin{defn}
    This is the same definition as before but replacing pseudo-randomness with \textit{unpredictability}.

    A PRG $(\mathrm{Setup}, G)$ is \textit{unpredictable} if, for any index $i \in \{0, \ldots, \ell(\lambda)\}$ and any efficient adversary $\mathcal{A} : \{0,1\}^{n} \to \{0,1\}$, we have that:
    \[
    \Big|
    \Pr_{k \gets \mathcal{U}(\{0, 1\}^{n(\lambda)} } \big[\mathcal{A}(G(k)_{|i}) = G(k)_{i+1}\big] - \frac{1}{2}
    \Big| = \mathrm{negl}(\lambda)
    .\]
  \end{defn}

  We can now prove that the two definitions are equivalent.

  \begin{thm}
    The two definitions of a PRG are equivalent.
  \end{thm}
  \begin{prv}
    To simplify, we will remove the security parameter from the notations.

    On one side, assume we have a predictor $\mathcal{A} : \{0,1\}^i \to \{0,1\}$ that succeeds in guessing $G(k)_{i+1}$ with non-negligible probability.
    We then construct a distinguisher $\mathcal{B}$ against pseudo-randomness as $\mathcal{B}$ receive a sample $x$ from either $\mathcal{D}_0 = \mathcal{U}(\{0,1\}^\ell)$ or $\mathcal{D}_1 = G(\mathcal{U}(\{0,1\}^n))$:
    algorithm $\mathcal{B}$ runs $\mathcal{A}$ on input  $x_{|i}$ and checks if $\mathcal{A}(x_{|i}) \overset ? = x_{i+1}$.
    In that case, $\mathcal{B}$ will return $1$; otherwise it returns $0$.
    What is the advantage of $\mathcal{B}$?
    \begin{align*}
      \mathrm{Adv}_{\mathcal{B}} &=
      \big| \Pr_{x \gets \mathcal{D}_1}[\mathcal{B}(x) = 1] - \overbrace{\Pr_{x \gets \mathcal{D}_0}[\mathcal{B}(x) = 1]}^{1 / 2} \big| \\
      &= \big| \Pr_{x \gets \mathcal{D}_1}[\mathcal{A}(x_{|i}) = x_{i+1}] - \frac{1}{2} \big|
    .\end{align*}
    This is the definition of the predictability advantage of $\mathcal{A}$ (which is non-negligible by assumption).

    Next, we will use a technique called an \textit{Hybrid Argument} (due to Yao in '82).
    Assume we have a distinguisher $\mathcal{A}$ such that \[
      \mathrm{Adv}_\mathcal{A} = \big| \Pr_{x \gets \mathcal{D}_1}[\mathcal{A}(x) = 1] - \Pr_{x \gets \mathcal{D}_0}[A(x) = 1]\big|
    \]
    is non-negligible, say $\mathrm{Adv}_\mathcal{A} \ge \varepsilon$.
    We then define $\ell + 1$ distributions $(\mathcal{D}_i)_{i = 0, \ldots, \ell}$ as
    \[
    \mathcal{D}_i := \mleft\{\,x \in \{0,1\}^\ell \;\middle|\;
      \begin{array}{l}
        x_{|i} = G(k)_{|i} \text{ for } k \gets \mathcal{U}(\{0,1\}^n)\\
        x_{|i+1, \ldots, \ell} \gets \mathcal{U}(\{0,1\}^{\ell - i})
      \end{array}
    \,\mright\} 
    .\]
    We then have, by all the terms cancelling (this is a telescoping sum), that:
    \begin{align*}
      \varepsilon \le \mathrm{Adv}_\mathcal{A}(\mathcal{D}_0, \mathcal{D}_n)
      &= \Big|\sum_{i = 0}^\ell \big(\Pr_{x \gets \mathcal{D}_{i+1}}[\mathcal{A}(x) = 1] - \Pr_{x \gets \mathcal{D}_i}[\mathcal{A}(x) = 1]\big)\Big| \\
      &\le \sum_{i = 0}^\ell \big|\Pr_{x \gets \mathcal{D}_{i+1}}[\mathcal{A}(x) = 1] - \Pr_{x \gets \mathcal{D}_i}[\mathcal{A}(x) = 1]\big|\\
      &\le \sum_{i= 0}^\ell \mathrm{Adv}_\mathcal{A}(\mathcal{D}_i, \mathcal{D}_{i+1})
    .\end{align*}
    By the pigeonhole principle, we have that there exists an $i \in \{0,\ldots,\ell\}$, such that \[
      \big|\Pr_{x \gets \mathcal{D}_{i+1}}[\mathcal{A}(x) = 1] - \Pr_{x \gets \mathcal{D}_i}[\mathcal{A}(x) = 1] \big| \ge \frac{\varepsilon}{\ell + 1}
    .\] 
    As $\varepsilon$ is non-negligible and  $\ell + 1$ being polynomial in $\lambda$, we have that $\varepsilon / (\ell + 1)$ is non-negligible.
    How to turn this into a predictor for $i$?
    Let us define $\mathcal{B}_i$ as a predictor which is given $G(k)_{|i}$ and supposed to predict $G(k)_{i+1}$.
    Algorithm $\mathcal{B}_i$ will computes $x \in \{0,1\}^\ell$ with $x \gets G(k)_{|i} \mathop{||} y $ where $y \gets \mathcal{U}(\{0,1\}^{\ell-i})$.
    Then $\mathcal{B}_i$ runs algorithms $\mathcal{A}$ on input $x$, and $\mathcal{A}$ returns a bit  $b \in \{0,1\}$ and $\mathcal{B}_i$ outputs a prediction $x_{i+1}$ for $G(k)_{i+1}$ if $b = 1$ and  $1 - x_{i+1}$ otherwise.
    What is the prediction advantage of $\mathcal{B}_i$?
    \begin{align*}
      &\phantom{={}}\Pr[\mathcal{B}_i(G(k)_{|i}) = G(k)_{i+1}]\\
      &= \Pr \left[
        \begin{array}{c}
          \mathcal{A}(x) = 0 \land x_{i+1} = 1 - G(k)_{i+1}\\
          \lor\\
          \mathcal{A}(x) = 1 \land x_{i+1} = G(k)_{i+1}
        \end{array}
      \right] \\
      &= \Pr_{x \gets \mathcal{D}_i} [\mathcal{A}(x) = 0 \land x_{i+1} = 1 - G(k)_{i+1}]\\
      &\quad + \Pr_{x\gets \mathcal{D}_i}[\mathcal{A}(x) = 1 \land x_{i+1} = G(k)_{i+1}]\\
      &=
      \frac{1}{2} \Pr_{x \gets \bar{\mathcal{D}}_{i+1}}[\mathcal{A}(x) = 0]
      + \frac{1}{2} \Pr_{x \gets \mathcal{D}_{i}}[\mathcal{A}(x) = 1]\\
      &= \frac{1}{2}\big(
        \Pr_{x \gets \mathcal{D}_{i+1}}[\mathcal{A}(x) = 1]
        + 1 - \Pr_{x \gets \bar{\mathcal{D}}_{i+1}}[\mathcal{A}(x) = 1]
      \big) \\
    .\end{align*}
    where we write $\bar{\mathcal{D}}_{i+1}$ is the "flipped" of $\mathcal{D}_{i+1}$.
  \end{prv}

\end{document}
